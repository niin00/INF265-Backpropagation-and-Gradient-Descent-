{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from tests_backpropagation import main_test\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class ``MyNet``\n",
    "\n",
    "Read carefully how ``MyNet`` is implemented in the cell below. In particular:  \n",
    "- ``n_hid`` is a list of integer, representing the number of hidden units in each hidden layer.   \n",
    "-  ``MyNet([2, 3, 2]) = MiniNet()`` where ``MiniNet`` is the neural network defined in the fourth tutorial, in which notations are also clarified.     \n",
    "- ``model.L`` is the number of hidden layers, ``L``   \n",
    "- ``model.f[l]`` is the activation function of layer ``l``, $f^{[l]}$ (here ``torch.tanh``)   \n",
    "- ``model.df[l]`` is the derivative of the activation function, $f'^{[l]}$   \n",
    "- ``model.a[l]``  is the tensor $A^{[l]}$, (shape: ``(1, n(l))``)   \n",
    "- ``model.z[l]``  is the tensor $Z^{[l]}$, (shape: ``(1, n(l))``)  \n",
    "- Weights $W^{[l]}$ (shape: ``(n(l+1), n(l))``) and biases $\\mathbf{b}^{[l]}$ (shape: ``(n(l+1))``) can be accessed as follows:\n",
    "```\n",
    "weights = model.fc[str(l)].weight.data\n",
    "bias = model.fc[str(l)].bias.data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, n_l = [2, 3, 2]):\n",
    "        super().__init__() \n",
    "        \n",
    "        \n",
    "        # number of layers in our network (following Andrew's notations)\n",
    "        self.L = len(n_l)-1\n",
    "        self.n_l = n_l\n",
    "        \n",
    "        # Where we will store our neuron values\n",
    "        # - z: before activation function \n",
    "        # - a: after activation function (a=f(z))\n",
    "        self.z = {i : None for i in range(1, self.L+1)}\n",
    "        self.a = {i : None for i in range(self.L+1)}\n",
    "\n",
    "        # Where we will store the gradients for our custom backpropagation algo\n",
    "        self.dL_dw = {i : None for i in range(1, self.L+1)}\n",
    "        self.dL_db = {i : None for i in range(1, self.L+1)}\n",
    "\n",
    "        # Our activation functions\n",
    "        self.f = {i : lambda x : torch.tanh(x) for i in range(1, self.L+1)}\n",
    "\n",
    "        # Derivatives of our activation functions\n",
    "        self.df = {\n",
    "            i : lambda x : (1 / (torch.cosh(x)**2)) \n",
    "            for i in range(1, self.L+1)\n",
    "        }\n",
    "        \n",
    "        # fully connected layers\n",
    "        # We have to use nn.ModuleDict and to use strings as keys here to \n",
    "        # respect pytorch requirements (otherwise, the model does not learn)\n",
    "        self.fc = nn.ModuleDict({str(i): None for i in range(1, self.L+1)})\n",
    "        for i in range(1, self.L+1):\n",
    "            self.fc[str(i)] = nn.Linear(in_features=n_l[i-1], out_features=n_l[i])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input layer\n",
    "        self.a[0] = torch.flatten(x, 1)\n",
    "        \n",
    "        # Hidden layers until output layer\n",
    "        for i in range(1, self.L+1):\n",
    "\n",
    "            # fully connected layer\n",
    "            self.z[i] = self.fc[str(i)](self.a[i-1])\n",
    "            # activation\n",
    "            self.a[i] = self.f[i](self.z[i])\n",
    "\n",
    "        # return output\n",
    "        return self.a[self.L]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Write a function ``backpropagation(model, y_true, y_pred)`` that computes:\n",
    "\n",
    "- $\\frac{\\partial L}{\\partial w^{[l]}_{i,j}}$ and store them in ``model.dL_dw[l][i,j]`` for $l \\in [1 .. L]$ \n",
    "- $\\frac{\\partial L}{\\partial b^{[l]}_{j}}$ and store them in ``model.dL_db[l][j]`` for $l \\in [1 .. L]$ \n",
    "\n",
    "assuming ``model`` is an instance of the ``MyNet`` class.\n",
    "\n",
    "A vectorized implementation would be appreciated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(model, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Manually compute gradients dL/dw and dL/db for each layer in MyNet.\n",
    "    Assumes batch_size = 1, so shapes are mostly (1, n[l]).\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        L = model.L\n",
    "        \n",
    "        #------------------------------------------------\n",
    "        # 1) Output-layer delta\n",
    "        #    delta_curr will stay shape (1, n[L]) if y_pred is also (1, n[L]) \n",
    "        #------------------------------------------------\n",
    "        delta_curr = dloss(y_true, y_pred) * model.df[L](model.z[L])\n",
    "        # Weight gradients: shape (n[L], n[L-1])\n",
    "        #   delta_curr.view(-1,1) is (n[L],1)\n",
    "        #   model.a[L-1] is (1,n[L-1]) => broadcast => (n[L], n[L-1])\n",
    "        model.dL_dw[L] = delta_curr.view(-1, 1) * model.a[L-1]\n",
    "        \n",
    "        # Bias gradients: shape (n[L],)\n",
    "        #   delta_curr is (1,n[L]), so delta_curr.squeeze(0) => (n[L],)\n",
    "        model.dL_db[L] = delta_curr.squeeze(0)\n",
    "        \n",
    "        #------------------------------------------------\n",
    "        # 2) Backprop for hidden layers\n",
    "        # move backward from l = L-1 down to l=1\n",
    "        #------------------------------------------------\n",
    "        for l in range(L-1, 0, -1):\n",
    "            # delta_prev = delta_curr\n",
    "            weights_prev = model.fc[str(l+1)].weight.data  # shape (n[l+1], n[l])\n",
    "            \n",
    "            #   delta_curr: (1, n[l+1]) => delta_curr.view(-1,1): (n[l+1],1)\n",
    "            #   weights_prev: (n[l+1], n[l]) => elementwise multiply => (n[l+1], n[l])\n",
    "            tmp = delta_curr.view(-1, 1) * weights_prev\n",
    "            \n",
    "            # sum over axis=0 => shape (n[l],). We'll keep dim so it remains (1, n[l])\n",
    "            tmp = tmp.sum(dim=0, keepdim=True)  # => (1, n[l])\n",
    "            \n",
    "            # multiply by derivative of activation => shape (1, n[l])\n",
    "            delta_curr = tmp * model.df[l](model.z[l])\n",
    "            \n",
    "            # Weight gradient => shape (n[l], n[l-1]) \n",
    "            model.dL_dw[l] = delta_curr.view(-1, 1) * model.a[l-1]\n",
    "            \n",
    "            # Bias gradient => shape (n[l],)\n",
    "            model.dL_db[l] = delta_curr.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the cells below, and check the output\n",
    "\n",
    "- In the 1st cell, we use a toy dataset and the same architecture as the MiniNet class of the fourth tutorial. \n",
    "- In the 2nd cell, we use a few samples of the MNIST dataset with a consistent model architecture (``24x24`` black and white cropped images as input and ``10`` output classes). \n",
    "\n",
    "You can set ``verbose`` to ``True`` if you want more details about your computations versus what is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n",
      "\n",
      " ====================== Epoch 1 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[ 6.7814e-08,  7.2355e-08],\n",
      "        [-1.0720e-03, -1.1438e-03],\n",
      "        [-2.2284e-03, -2.3776e-03]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[ 6.7814e-08,  7.2355e-08],\n",
      "        [-1.0720e-03, -1.1438e-03],\n",
      "        [-2.2284e-03, -2.3776e-03]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([ 8.0769e-09, -1.2768e-04, -2.6541e-04])\n",
      "  Autograd's computation:\n",
      " tensor([ 8.0769e-09, -1.2768e-04, -2.6541e-04])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " ====================== Epoch 2 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[ 3.5432e-08,  3.7805e-08],\n",
      "        [-4.1697e-04, -4.4489e-04],\n",
      "        [-1.0000e-03, -1.0670e-03]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[ 3.5432e-08,  3.7805e-08],\n",
      "        [-4.1697e-04, -4.4489e-04],\n",
      "        [-1.0000e-03, -1.0670e-03]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([ 4.2201e-09, -4.9663e-05, -1.1910e-04])\n",
      "  Autograd's computation:\n",
      " tensor([ 4.2201e-09, -4.9663e-05, -1.1910e-04])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " ====================== Epoch 3 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[ 2.5834e-08,  2.7564e-08],\n",
      "        [-2.4624e-04, -2.6274e-04],\n",
      "        [-6.4282e-04, -6.8587e-04]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[ 2.5834e-08,  2.7564e-08],\n",
      "        [-2.4624e-04, -2.6274e-04],\n",
      "        [-6.4282e-04, -6.8587e-04]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([ 3.0769e-09, -2.9329e-05, -7.6563e-05])\n",
      "  Autograd's computation:\n",
      " tensor([ 3.0769e-09, -2.9329e-05, -7.6563e-05])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " ====================== Epoch 4 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[ 2.1521e-08,  2.2962e-08],\n",
      "        [-1.7231e-04, -1.8385e-04],\n",
      "        [-4.7603e-04, -5.0791e-04]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[ 2.1521e-08,  2.2962e-08],\n",
      "        [-1.7231e-04, -1.8385e-04],\n",
      "        [-4.7603e-04, -5.0791e-04]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([ 2.5632e-09, -2.0523e-05, -5.6697e-05])\n",
      "  Autograd's computation:\n",
      " tensor([ 2.5632e-09, -2.0523e-05, -5.6697e-05])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " ====================== Epoch 5 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[ 1.9401e-08,  2.0700e-08],\n",
      "        [-1.3261e-04, -1.4149e-04],\n",
      "        [-3.8135e-04, -4.0689e-04]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[ 1.9401e-08,  2.0700e-08],\n",
      "        [-1.3261e-04, -1.4149e-04],\n",
      "        [-3.8135e-04, -4.0689e-04]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([ 2.3107e-09, -1.5794e-05, -4.5420e-05])\n",
      "  Autograd's computation:\n",
      " tensor([ 2.3107e-09, -1.5794e-05, -4.5420e-05])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " TEST PASSED: Gradients consistent with autograd's computations.\n",
      "\n",
      " TEST PASSED: Gradients consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "tensor([[-0.3581, -0.7438],\n",
      "        [ 0.5436, -0.0012],\n",
      "        [ 0.1644,  0.2690]])\n",
      "tensor([0.0704, 0.3112, 0.7295])\n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 4 / 4: ALL TEST PASSED :)\n"
     ]
    }
   ],
   "source": [
    "model = MyNet([2, 3, 2])\n",
    "main_test(backpropagation, model, verbose=True, data='toy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n",
      "\n",
      " ====================== Epoch 1 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[  1.7421e-27,   1.7421e-27,   1.7421e-27,  ...,   1.7421e-27,\n",
      "           1.7421e-27,   1.7421e-27],\n",
      "        [-5.9696e-177, -5.9696e-177, -5.9696e-177,  ..., -5.9696e-177,\n",
      "         -5.9696e-177, -5.9696e-177],\n",
      "        [ -3.7006e-19,  -3.7006e-19,  -3.7006e-19,  ...,  -3.7006e-19,\n",
      "          -3.7006e-19,  -3.7006e-19],\n",
      "        ...,\n",
      "        [-1.6763e-132, -1.6763e-132, -1.6763e-132,  ..., -1.6763e-132,\n",
      "         -1.6763e-132, -1.6763e-132],\n",
      "        [  1.4466e-40,   1.4466e-40,   1.4466e-40,  ...,   1.4466e-40,\n",
      "           1.4466e-40,   1.4466e-40],\n",
      "        [  9.5900e-56,   9.5900e-56,   9.5900e-56,  ...,   9.5900e-56,\n",
      "           9.5900e-56,   9.5900e-56]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "        [-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "        ...,\n",
      "        [-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([-4.1084e-27, 1.4078e-176,  8.7273e-19, -2.0038e+01,  2.7808e-77,\n",
      "         6.5821e-18, -3.4859e-43, -7.9225e-37, -3.4193e-25,  4.3084e-25,\n",
      "        -1.1987e-24, -2.1216e-34, -2.4377e-30, 3.9534e-132, -3.4116e-40,\n",
      "        -2.2617e-55])\n",
      "  Autograd's computation:\n",
      " tensor([  0.0000,   0.0000,   0.0000, -20.0377,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " TEST PASSED: Gradients consistent with autograd's computations.\n",
      "\n",
      " TEST PASSED: Gradients consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "tensor([[-0.2101, -0.2493, -0.1720,  ..., -0.1747, -0.2023, -0.2544],\n",
      "        [ 0.6405,  0.6012,  0.6404,  ...,  0.6005,  0.6012,  0.6509],\n",
      "        [ 0.2595,  0.2065,  0.2728,  ...,  0.2550,  0.2008,  0.2776],\n",
      "        ...,\n",
      "        [ 0.4555,  0.4385,  0.5049,  ...,  0.4893,  0.4710,  0.4452],\n",
      "        [ 0.0960,  0.1214,  0.0961,  ...,  0.0811,  0.1427,  0.1286],\n",
      "        [-1.0650, -1.0229, -1.0307,  ..., -1.0668, -1.0503, -1.0247]])\n",
      "tensor([ 0.4872, -1.4792, -0.5399,  2.7849, -0.6366, -1.5495, -0.3878,  0.3257,\n",
      "         0.8094, -1.5389,  1.3450,  0.2497,  0.1751, -1.1013, -0.2663,  2.4902])\n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 4 / 4: ALL TEST PASSED :)\n"
     ]
    }
   ],
   "source": [
    "model = MyNet([24*24, 16, 10])\n",
    "main_test(backpropagation, model, verbose=True, data='mnist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphapp-CGLyoVIh-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "cd17699b0f8c8ba78222bc090e4903241cbfccfe2c3d9b347e7b5dfb6ba74475"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
