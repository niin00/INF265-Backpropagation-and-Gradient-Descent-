{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the same results with train and train_manual_update\n",
    "- Write torch.manual_seed(42) at the beginning of your notebook.\n",
    "- Write torch.set_default_dtype(torch.double) at the beginning of your notebook to alleviate precision errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "Load, analyse and preprocess the CIFAR-10 dataset. Split it into 3\n",
    "datasets: training, validation and test. Take a subset of these datasets\n",
    "by keeping only 2 labels: cat and car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training set size: 9017\n",
      "Validation set size: 983\n",
      "Test set size: 2000\n"
     ]
    }
   ],
   "source": [
    "# Select the available device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def load_cifar(train_val_split=0.9, data_path='../data/', preprocessor=None):\n",
    "    \"\"\"\n",
    "    Loads the CIFAR-10 dataset, splits it into train/validation/test subsets, and\n",
    "    reduces it to only two classes (cat and car). Returns three lists corresponding\n",
    "    to the training, validation, and test datasets.\n",
    "\n",
    "    Args:\n",
    "        train_val_split (float): The fraction of the dataset to use for training (the remainder is for validation).\n",
    "        data_path (str): The directory path where the CIFAR-10 dataset should be downloaded/stored.\n",
    "        preprocessor (torchvision.transforms.Compose): Transformations to apply to images upon loading.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_data, val_data, test_data), each a list of (image, label) pairs,\n",
    "               where label is 0 for 'cat' and 1 for 'car'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If no preprocessor is specified, define a default set of transforms\n",
    "    if preprocessor is None:\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.Resize((16, 16)),  # Resize images to 16x16\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=(0.4915, 0.4823, 0.4468),\n",
    "                std=(0.2470, 0.2435, 0.2616)\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    # Load the full CIFAR-10 dataset for training+validation\n",
    "    data_train_val = datasets.CIFAR10(\n",
    "        data_path, train=True, download=True, transform=preprocessor\n",
    "    )\n",
    "    # Load the full CIFAR-10 dataset for testing\n",
    "    data_test = datasets.CIFAR10(\n",
    "        data_path, train=False, download=True, transform=preprocessor\n",
    "    )\n",
    "\n",
    "    # Calculate the split sizes for training and validation\n",
    "    n_train = int(len(data_train_val) * train_val_split)\n",
    "    n_val = len(data_train_val) - n_train\n",
    "    \n",
    "    # Split the dataset into train and validation, using a fixed random seed for reproducibility\n",
    "    data_train, data_val = random_split(\n",
    "        data_train_val, [n_train, n_val],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    # only keep images labeled as \"car\" (label=1) or \"cat\" (label=3) in the original CIFAR-10\n",
    "    # Map them to labels: car -> 0, cat -> 1\n",
    "    label_map = {1: 0, 3: 1}\n",
    "    \n",
    "    # Create filtered training subset with only cars and cats\n",
    "    cifar_cars_cats_train = [\n",
    "        (img, label_map[label])\n",
    "        for (img, label) in data_train\n",
    "        if label in [1, 3]\n",
    "    ]\n",
    "    # Similarly for validation\n",
    "    cifar_cars_cats_val = [\n",
    "        (img, label_map[label])\n",
    "        for (img, label) in data_val\n",
    "        if label in [1, 3]\n",
    "    ]\n",
    "    # And for testing\n",
    "    cifar_cars_cats_test = [\n",
    "        (img, label_map[label])\n",
    "        for (img, label) in data_test\n",
    "        if label in [1, 3]\n",
    "    ]\n",
    "    \n",
    "    # Print dataset sizes for sanity check\n",
    "    print(\"Training set size:\", len(cifar_cars_cats_train))\n",
    "    print(\"Validation set size:\", len(cifar_cars_cats_val))\n",
    "    print(\"Test set size:\", len(cifar_cars_cats_test))\n",
    "\n",
    "    return cifar_cars_cats_train, cifar_cars_cats_val, cifar_cars_cats_test\n",
    "\n",
    "# Load the filtered CIFAR-10 datasets\n",
    "data_train, data_val, data_test = load_cifar()\n",
    "\n",
    "def compute_accuracy(model, loader):\n",
    "    \"\"\"\n",
    "    Computes the classification accuracy of a given PyTorch model on a specified dataloader.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to evaluate.\n",
    "        loader (DataLoader): A PyTorch DataLoader providing (images, labels) batches.\n",
    "\n",
    "    Returns:\n",
    "        float: The computed accuracy (0.0 to 1.0).\n",
    "    \"\"\"\n",
    "    model.eval()  # Switch model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # don't need gradients when simply running inference\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            # Move data and labels to the same device as the model\n",
    "            imgs = imgs.to(device=device, dtype=torch.double)\n",
    "            labels = labels.to(device=device)\n",
    "            \n",
    "            # Forward pass: get the predictions\n",
    "            outputs = model(imgs)\n",
    "            # Convert raw logit outputs to predicted class indices\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += int((predicted == labels).sum())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print('Accuracy: {:.2f}'.format(accuracy))\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a MyMLP class that implements a MLP in PyTorch (so only fully\n",
    "connected layers) such that:\n",
    "    \n",
    "    - The input dimension is 768(= 16 ∗ 16 ∗ 3) and the output dimension is 2 (for the 2 classes).\n",
    "    - The hidden layers have respectively 128 and 32 hidden units.\n",
    "    - All activation functions are ReLU. The last layer has no activation function since the cross-entropy loss already includes a softmax activation\n",
    "function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(16*16*3, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.flatten(x, start_dim=1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))    #no softmax, cross-entropy handles that\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out     \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a train(n_epochs, optimizer, model, loss_fn, train_loader) function that trains model for n_epochs epochs given an optimizer optimizer, a loss function loss_fn and a dataloader train_loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    \n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        \n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device, dtype=torch.double)\n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate the loss from this batch\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        # After the entire epoch, average out\n",
    "        loss_train /= n_batch\n",
    "        losses_train.append(loss_train)\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train))\n",
    "\n",
    "    return losses_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a similar function train manual_update that has no optimizer parameter, but a learning rate lr parameter instead and that manually updates each trainable parameter of model using equation (2). Do not forget to zero out all gradients after each iteration. \n",
    "\n",
    "Train 2 instances of MyMLP, one using train and the other using train_manual_update (use the same parameter values for both models). Compare their respective training losses. To get exactly the same results with both functions, see section 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_manual_update(n_epochs, model, loss_fn, train_loader, \n",
    "                        lr=1e-2, momentum_coeff=0., weight_decay=0.):\n",
    "    model.train()\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    \n",
    "    dict_mom = {}  # for momentum terms\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        \n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device, dtype=torch.double)\n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            # Forward\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Backward\n",
    "            loss.backward()\n",
    "\n",
    "            # Manual updates\n",
    "            with torch.no_grad():\n",
    "                for name, p in model.named_parameters():\n",
    "                    if p.grad is not None:\n",
    "                        grad = p.grad\n",
    "\n",
    "                        # L2 regularization\n",
    "                        if weight_decay:\n",
    "                            grad = grad + weight_decay * p.data\n",
    "\n",
    "                        # Momentum\n",
    "                        if momentum_coeff:\n",
    "                            if name in dict_mom:\n",
    "                                grad = grad + momentum_coeff * dict_mom[name]\n",
    "                            dict_mom[name] = grad\n",
    "\n",
    "                        # Update the parameter\n",
    "                        p.data = p.data - lr * grad\n",
    "\n",
    "                        # Reset the gradient\n",
    "                        p.grad.zero_()\n",
    "\n",
    "            # Accumulate this batch’s loss\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        # Average over batches\n",
    "        loss_train /= n_batch\n",
    "        losses_train.append(loss_train)\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train))\n",
    "\n",
    "    return losses_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n",
      "\n",
      "   Global parameters:\n",
      "batch_size =  256\n",
      "n_epoch =  30\n",
      "loss_fn =  CrossEntropyLoss()\n",
      "seed =  265\n",
      "\n",
      " =========================================== \n",
      "   Current parameters: \n",
      "lr = 0.01\n",
      "mom = 0\n",
      "decay = 0\n",
      "\n",
      " Using Pytorch's SGD \n",
      "00:32:20.534162  |  Epoch 1  |  Training loss 0.67882\n",
      "00:32:20.651903  |  Epoch 5  |  Training loss 0.56008\n",
      "00:32:20.805914  |  Epoch 10  |  Training loss 0.43988\n",
      "00:32:20.942234  |  Epoch 15  |  Training loss 0.37431\n",
      "00:32:21.088623  |  Epoch 20  |  Training loss 0.32573\n",
      "00:32:21.233918  |  Epoch 25  |  Training loss 0.29081\n",
      "00:32:21.383780  |  Epoch 30  |  Training loss 0.26554\n",
      "\n",
      " Accuracies \n",
      "Training\n",
      "Accuracy: 0.90\n",
      "Validation\n",
      "Accuracy: 0.87\n",
      "\n",
      " Using manual update\n",
      "00:32:21.427262  |  Epoch 1  |  Training loss 0.67882\n",
      "00:32:21.536881  |  Epoch 5  |  Training loss 0.56008\n",
      "00:32:21.673672  |  Epoch 10  |  Training loss 0.43988\n",
      "00:32:21.810336  |  Epoch 15  |  Training loss 0.37431\n",
      "00:32:21.955657  |  Epoch 20  |  Training loss 0.32573\n",
      "00:32:22.090796  |  Epoch 25  |  Training loss 0.29081\n",
      "00:32:22.231829  |  Epoch 30  |  Training loss 0.26554\n",
      "\n",
      " --- Accuracies --- \n",
      "TrainingAccuracy: 0.90\n",
      "ValidationAccuracy: 0.87\n",
      "\n",
      " =========================================== \n",
      "   Current parameters: \n",
      "lr = 0.01\n",
      "mom = 0\n",
      "decay = 0.01\n",
      "\n",
      " Using Pytorch's SGD \n",
      "00:32:22.279072  |  Epoch 1  |  Training loss 0.67891\n",
      "00:32:22.390602  |  Epoch 5  |  Training loss 0.56360\n",
      "00:32:22.544496  |  Epoch 10  |  Training loss 0.44577\n",
      "00:32:22.697324  |  Epoch 15  |  Training loss 0.38174\n",
      "00:32:22.851956  |  Epoch 20  |  Training loss 0.33441\n",
      "00:32:23.012963  |  Epoch 25  |  Training loss 0.30002\n",
      "00:32:23.173914  |  Epoch 30  |  Training loss 0.27528\n",
      "\n",
      " Accuracies \n",
      "Training\n",
      "Accuracy: 0.89\n",
      "Validation\n",
      "Accuracy: 0.87\n",
      "\n",
      " Using manual update\n",
      "00:32:23.220236  |  Epoch 1  |  Training loss 0.67891\n",
      "00:32:23.341313  |  Epoch 5  |  Training loss 0.56360\n",
      "00:32:23.499931  |  Epoch 10  |  Training loss 0.44577\n",
      "00:32:23.663899  |  Epoch 15  |  Training loss 0.38174\n",
      "00:32:23.824981  |  Epoch 20  |  Training loss 0.33441\n",
      "00:32:23.961872  |  Epoch 25  |  Training loss 0.30002\n",
      "00:32:24.118721  |  Epoch 30  |  Training loss 0.27528\n",
      "\n",
      " --- Accuracies --- \n",
      "TrainingAccuracy: 0.89\n",
      "ValidationAccuracy: 0.87\n",
      "\n",
      " =========================================== \n",
      "   Current parameters: \n",
      "lr = 0.01\n",
      "mom = 0.9\n",
      "decay = 0\n",
      "\n",
      " Using Pytorch's SGD \n",
      "00:32:24.174121  |  Epoch 1  |  Training loss 0.61956\n",
      "00:32:24.308064  |  Epoch 5  |  Training loss 0.26257\n",
      "00:32:24.477307  |  Epoch 10  |  Training loss 0.16050\n",
      "00:32:24.647676  |  Epoch 15  |  Training loss 0.10025\n",
      "00:32:24.795831  |  Epoch 20  |  Training loss 0.08245\n",
      "00:32:24.941853  |  Epoch 25  |  Training loss 0.14350\n",
      "00:32:25.079437  |  Epoch 30  |  Training loss 0.03859\n",
      "\n",
      " Accuracies \n",
      "Training\n",
      "Accuracy: 0.98\n",
      "Validation\n",
      "Accuracy: 0.91\n",
      "\n",
      " Using manual update\n",
      "00:32:25.122791  |  Epoch 1  |  Training loss 0.61991\n",
      "00:32:25.236882  |  Epoch 5  |  Training loss 0.26263\n",
      "00:32:25.391811  |  Epoch 10  |  Training loss 0.16026\n",
      "00:32:25.533411  |  Epoch 15  |  Training loss 0.10172\n",
      "00:32:25.678447  |  Epoch 20  |  Training loss 0.07247\n",
      "00:32:25.833505  |  Epoch 25  |  Training loss 0.10451\n",
      "00:32:26.013414  |  Epoch 30  |  Training loss 0.08265\n",
      "\n",
      " --- Accuracies --- \n",
      "TrainingAccuracy: 0.96\n",
      "ValidationAccuracy: 0.90\n",
      "\n",
      " =========================================== \n",
      "   Current parameters: \n",
      "lr = 0.01\n",
      "mom = 0.9\n",
      "decay = 0.01\n",
      "\n",
      " Using Pytorch's SGD \n",
      "00:32:26.073421  |  Epoch 1  |  Training loss 0.62101\n",
      "00:32:26.210326  |  Epoch 5  |  Training loss 0.27125\n",
      "00:32:26.401783  |  Epoch 10  |  Training loss 0.17891\n",
      "00:32:26.577146  |  Epoch 15  |  Training loss 0.12759\n",
      "00:32:26.736419  |  Epoch 20  |  Training loss 0.09610\n",
      "00:32:26.896186  |  Epoch 25  |  Training loss 0.07428\n",
      "00:32:27.045529  |  Epoch 30  |  Training loss 0.07514\n",
      "\n",
      " Accuracies \n",
      "Training\n",
      "Accuracy: 0.97\n",
      "Validation\n",
      "Accuracy: 0.90\n",
      "\n",
      " Using manual update\n",
      "00:32:27.098105  |  Epoch 1  |  Training loss 0.62101\n",
      "00:32:27.222260  |  Epoch 5  |  Training loss 0.27125\n",
      "00:32:27.380624  |  Epoch 10  |  Training loss 0.17891\n",
      "00:32:27.572706  |  Epoch 15  |  Training loss 0.12759\n",
      "00:32:27.759748  |  Epoch 20  |  Training loss 0.09610\n",
      "00:32:27.938473  |  Epoch 25  |  Training loss 0.07428\n",
      "00:32:28.119494  |  Epoch 30  |  Training loss 0.07514\n",
      "\n",
      " --- Accuracies --- \n",
      "TrainingAccuracy: 0.97\n",
      "ValidationAccuracy: 0.90\n",
      "\n",
      " =========================================== \n",
      "   Current parameters: \n",
      "lr = 0.01\n",
      "mom = 0.9\n",
      "decay = 0.001\n",
      "\n",
      " Using Pytorch's SGD \n",
      "00:32:28.174285  |  Epoch 1  |  Training loss 0.61969\n",
      "00:32:28.298562  |  Epoch 5  |  Training loss 0.26313\n",
      "00:32:28.453686  |  Epoch 10  |  Training loss 0.16142\n",
      "00:32:28.599402  |  Epoch 15  |  Training loss 0.10214\n",
      "00:32:28.764782  |  Epoch 20  |  Training loss 0.08523\n",
      "00:32:28.913983  |  Epoch 25  |  Training loss 0.15923\n",
      "00:32:29.069756  |  Epoch 30  |  Training loss 0.05469\n",
      "\n",
      " Accuracies \n",
      "Training\n",
      "Accuracy: 0.97\n",
      "Validation\n",
      "Accuracy: 0.89\n",
      "\n",
      " Using manual update\n",
      "00:32:29.120284  |  Epoch 1  |  Training loss 0.61969\n",
      "00:32:29.251967  |  Epoch 5  |  Training loss 0.26313\n",
      "00:32:29.406744  |  Epoch 10  |  Training loss 0.16142\n",
      "00:32:29.559556  |  Epoch 15  |  Training loss 0.10214\n",
      "00:32:29.713636  |  Epoch 20  |  Training loss 0.08523\n",
      "00:32:29.880634  |  Epoch 25  |  Training loss 0.15923\n",
      "00:32:30.037463  |  Epoch 30  |  Training loss 0.05469\n",
      "\n",
      " --- Accuracies --- \n",
      "TrainingAccuracy: 0.97\n",
      "ValidationAccuracy: 0.89\n",
      "\n",
      " =========================================== \n",
      "   Current parameters: \n",
      "lr = 0.01\n",
      "mom = 0.8\n",
      "decay = 0.01\n",
      "\n",
      " Using Pytorch's SGD \n",
      "00:32:30.087280  |  Epoch 1  |  Training loss 0.64253\n",
      "00:32:30.212102  |  Epoch 5  |  Training loss 0.33420\n",
      "00:32:30.378789  |  Epoch 10  |  Training loss 0.23614\n",
      "00:32:30.542862  |  Epoch 15  |  Training loss 0.18365\n",
      "00:32:30.685712  |  Epoch 20  |  Training loss 0.14702\n",
      "00:32:30.833189  |  Epoch 25  |  Training loss 0.12131\n",
      "00:32:30.967707  |  Epoch 30  |  Training loss 0.10207\n",
      "\n",
      " Accuracies \n",
      "Training\n",
      "Accuracy: 0.97\n",
      "Validation\n",
      "Accuracy: 0.91\n",
      "\n",
      " Using manual update\n",
      "00:32:31.013419  |  Epoch 1  |  Training loss 0.64253\n",
      "00:32:31.123306  |  Epoch 5  |  Training loss 0.33420\n",
      "00:32:31.262781  |  Epoch 10  |  Training loss 0.23614\n",
      "00:32:31.399788  |  Epoch 15  |  Training loss 0.18365\n",
      "00:32:31.540098  |  Epoch 20  |  Training loss 0.14702\n",
      "00:32:31.690055  |  Epoch 25  |  Training loss 0.12131\n",
      "00:32:31.841582  |  Epoch 30  |  Training loss 0.10207\n",
      "\n",
      " --- Accuracies --- \n",
      "TrainingAccuracy: 0.97\n",
      "ValidationAccuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "# Define training hyperparameters\n",
    "n_epochs = 30          # number of epochs\n",
    "batch_size = 256       # batch size for training\n",
    "seed = 265             # random seed for reproducibility\n",
    "\n",
    "# Create DataLoaders for training and validation (no shuffling to keep data order deterministic)\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(data_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the loss function (cross-entropy for classification)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Lists of possible hyperparameter values for learning rate, momentum, and weight decay\n",
    "list_lr = [0.01]*6\n",
    "list_momentum = [0, 0, 0.9, 0.9, 0.9, 0.8]\n",
    "list_decay = [0, 0.01, 0, 0.01, 0.001, 0.01]\n",
    "\n",
    "# Construct a list of parameter dictionaries for each run\n",
    "params = [{\n",
    "        \"lr\": list_lr[i],\n",
    "        \"mom\": list_momentum[i],\n",
    "        \"decay\": list_decay[i],\n",
    "    } for i in range(len(list_lr))\n",
    "]\n",
    "\n",
    "print(\"\\n   Global parameters:\")\n",
    "print(\"batch_size = \", batch_size)\n",
    "print(\"n_epoch = \", n_epochs)\n",
    "print(\"loss_fn = \", nn.CrossEntropyLoss())\n",
    "print(\"seed = \", seed)\n",
    "\n",
    "# These lists will store accuracies and model references so that we can pick the best model later\n",
    "accuracies = []\n",
    "models = []\n",
    "\n",
    "# Iterate over all parameter configurations\n",
    "for i in range(len(list_lr)):\n",
    "    \n",
    "    print(\"\\n =========================================== \")\n",
    "    print(\"   Current parameters: \")\n",
    "    # Print current hyperparameters in a readable format\n",
    "    print(\"\".join(['%s = %s\\n' % (key, value) for (key, value) in params[i].items()]))\n",
    "    \n",
    "    print(\" Using Pytorch's SGD \")\n",
    "    \n",
    "    # Set a fixed random seed to ensure reproducible initial weights\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Initialize the model and move it to the chosen device\n",
    "    model = MyNet()\n",
    "    model.to(device=device)\n",
    "\n",
    "    # Create a built-in PyTorch SGD optimizer with the current hyperparameters\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(), \n",
    "        lr=params[i][\"lr\"], \n",
    "        momentum=params[i][\"mom\"], \n",
    "        weight_decay=params[i][\"decay\"]\n",
    "    )\n",
    "\n",
    "    # Train the model using the built-in optimizer\n",
    "    loss_train = train(\n",
    "        n_epochs=n_epochs,\n",
    "        optimizer=optimizer,\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        train_loader=train_loader,\n",
    "    )\n",
    "    \n",
    "    # Compute and print accuracies for the built-in-optimizer-trained model\n",
    "    print(\"\\n Accuracies \")\n",
    "    print(\"Training\")\n",
    "    acc = compute_accuracy(model, train_loader)\n",
    "    print(\"Validation\")\n",
    "    acc_val = compute_accuracy(model, val_loader)\n",
    "\n",
    "    # Now train a new model using manual updates (same hyperparameters)\n",
    "    print(\"\\n Using manual update\")\n",
    "    torch.manual_seed(seed)  # Ensure same initial weights\n",
    "    model_manual = MyNet()\n",
    "    model_manual.to(device=device)\n",
    "\n",
    "    loss_train_manual = train_manual_update(\n",
    "        n_epochs=n_epochs,\n",
    "        model=model_manual,\n",
    "        loss_fn=loss_fn,\n",
    "        train_loader=train_loader,\n",
    "        lr=params[i][\"lr\"],\n",
    "        momentum_coeff=params[i][\"mom\"], \n",
    "        weight_decay=params[i][\"decay\"],\n",
    "    )\n",
    "\n",
    "    # Compute and print accuracies for the model trained with manual updates\n",
    "    print(\"\\n --- Accuracies --- \")\n",
    "    print(\"Training\", end=\"\")\n",
    "    acc_manual = compute_accuracy(model_manual, train_loader)\n",
    "    print(\"Validation\", end=\"\")\n",
    "    acc_manual_val = compute_accuracy(model_manual, val_loader)\n",
    "    \n",
    "    # model selection:\n",
    "    # 1) Store the validation accuracy from the built-in SGD model\n",
    "    accuracies.append(acc)\n",
    "    models.append(model)\n",
    "\n",
    "    # 2) Store the validation accuracy from the manually updated model\n",
    "    accuracies.append(acc_manual)\n",
    "    models.append(model_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The best model was trained with \n",
      "    lr = 0.01\n",
      "    mom = 0\n",
      "    decay = 0\n",
      "Training accuracy of the best model: \n",
      "Accuracy: 0.97\n",
      "Validation accuracy of the best model: \n",
      "Accuracy: 0.91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9094608341810784"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_best_model = np.argmax(accuracies)\n",
    "best_model = models[i_best_model]\n",
    "\n",
    "params_best_model = params[i_best_model]\n",
    "\n",
    "print(\n",
    "    \"\\nThe best model was trained with\",\n",
    "    \"\".join(['\\n    %s = %s' % (key, value) for (key, value) in params[i_best_model].items()]))\n",
    "\n",
    "print(\"Training accuracy of the best model: \")\n",
    "compute_accuracy(best_model, train_loader)\n",
    "print(\"Validation accuracy of the best model: \")\n",
    "compute_accuracy(best_model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy score of the best model\n",
      "Accuracy: 0.91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(data_test, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "print('Test Accuracy score of the best model')\n",
    "compute_accuracy(best_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
